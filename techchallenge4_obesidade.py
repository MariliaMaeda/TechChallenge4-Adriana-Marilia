# -*- coding: utf-8 -*-
"""TechChallenge4 - Obesidade.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_RTM3JUgCISCvX5Ex8rWOU5PDWvYVi-
"""

# @title Diagn√≥stico de Obesidade com IA | Contexto e Importa√ß√£o
# Neste projeto,# n√≥s, como p√≥s-graduandas de Data Analytics contratadas por um hospital,
# temos um objetivo: prever o n√≠vel de obesidade de pacientes com alta precis√£o,
# apoiando diagn√≥sticos cl√≠nicos.
# A primeira etapa √© importar os dados e realizar uma leitura inicial.

import pandas as pd

# Carregamos o dataset entregue pela institui√ß√£o de sa√∫de
obesidade = pd.read_csv('/content/Obesity.csv')

# Inspe√ß√£o 1: conhecendo as primeiras entradas do banco
obesidade.head()

# @title # Antes de propor um modelo preditivo | Inspe√ß√£o dos dados.
# Fundamental conhecer os dados dispon√≠veis: tipos, aus√™ncia de valores, estrutura geral.

# Tipos de dados e presen√ßa de nulos
obesidade.info()

# Estat√≠sticas descritivas para vari√°veis num√©ricas
obesidade.describe()

# Verifica√ß√£o de valores nulos por coluna
valores_nulos = obesidade.isnull().sum()
print("Valores nulos por coluna:\n", valores_nulos)

# @title Transforma√ß√£o de dados comportamentais em valores num√©ricos
#Muitos dos dados comportamentais foram coletados como 'sim' ou 'n√£o'.
#Para que o algoritmo consiga compreender essas informa√ß√µes, precisamos transform√°-las em valores num√©ricos.
#como por exemplo, transformar 'sim' em 1 e 'n√£o' em 0. Isso √© uma etapa b√°sica na prepara√ß√£o de dados para ML.

# Convertendo vari√°veis booleanas de comportamento para formato num√©rico

obesidade['Gender'] = obesidade['Gender'].replace({'Female': 0, 'Male': 1}).astype(int)  # G√™nero codificado
obesidade['family_history'] = obesidade['family_history'].replace({'no': 0, 'yes': 1}).astype(int)  # Hist√≥rico familiar
obesidade['FAVC'] = obesidade['FAVC'].replace({'no': 0, 'yes': 1}).astype(int)  # Alimentos cal√≥ricos
obesidade['SMOKE'] = obesidade['SMOKE'].replace({'no': 0, 'yes': 1}).astype(int)  # Fuma
obesidade['SCC'] = obesidade['SCC'].replace({'no': 0, 'yes': 1}).astype(int)  # Controla calorias

# @title  Verifica√ß√£o e remo√ß√£o de duplicatas nos dados dos pacientes
# **na aus√™ncia de um ID de paciente**, para evitar vi√©s.

# Verificando duplicatas no DataFrame
print("Duplicatas antes:", obesidade.duplicated().sum())

# Removendo registros duplicados
obesidade = obesidade.drop_duplicates()

# Verificando ap√≥s remo√ß√£o
print("Duplicatas ap√≥s:", obesidade.duplicated().sum())

# @title Tradu√ß√£o das colunas do conjunto de dados
# Adequando ao jarg√£o de ambientes hospitalares. Aqui renomeamos as vari√°veis para portugu√™s, respeitando os nomes usuais utilizados por m√©dicos e nutricionistas.

obesidade.rename(columns={
    'Gender': 'G√™nero',
    'Age': 'Idade',
    'Height': 'Altura (m)',
    'Weight': 'Peso (kg)',
    'family_history': 'Hist√≥rico Familiar de Obesidade',
    'FAVC': 'Consome Alimentos Cal√≥ricos',
    'FCVC': 'Consome Vegetais',
    'NCP': 'Refei√ß√µes por Dia',
    'CAEC': 'Lanches entre Refei√ß√µes',
    'SMOKE': 'Fuma',
    'CH2O': '√Ågua Di√°ria',
    'SCC': 'Controla Calorias',
    'FAF': 'Frequ√™ncia de Atividade F√≠sica',
    'TUE': 'Tempo com Dispositivos Eletr√¥nicos',
    'CALC': 'Consumo de √Ålcool',
    'MTRANS': 'Meio de Transporte',
    'Obesity': 'N√≠vel de Obesidade'
}, inplace=True)

# @title Tradu√ß√£o dos r√≥tulos da vari√°vel alvo para o portugu√™s
#Para facilitar a leitura e a comunica√ß√£o com os profissionais da sa√∫de, traduzimos os r√≥tulos da vari√°vel alvo para o portugu√™s.

traducoes_obesidade = {
    "Insufficient_Weight": "Abaixo do Peso",
    "Normal_Weight": "Peso Normal",
    "Overweight_Level_I": "Sobrepeso I",
    "Overweight_Level_II": "Sobrepeso II",
    "Obesity_Type_I": "Obesidade Tipo I",
    "Obesity_Type_II": "Obesidade Tipo II",
    "Obesity_Type_III": "Obesidade Tipo III"
}

# ‚úÖ Aplicando a tradu√ß√£o diretamente na coluna correta
obesidade["N√≠vel de Obesidade"] = obesidade["N√≠vel de Obesidade"].replace(traducoes_obesidade)

# Verificando o resultado
obesidade["N√≠vel de Obesidade"].value_counts()

# @title
obesidade.head()

# @title Tradu√ß√µes para uniformizar e tornar o painel anal√≠tico mais intuitivo.
# Tradu√ß√£o de categorias
obesidade["Lanches entre Refei√ß√µes"] = obesidade["Lanches entre Refei√ß√µes"].replace({
    "Sometimes": "Pouco", "Frequently": "Frequentemente", "Always": "Sempre", "no": "Nunca"
})

obesidade["Consumo de √Ålcool"] = obesidade["Consumo de √Ålcool"].replace({
    "Sometimes": "Pouco", "Frequently": "Frequentemente", "Always": "Sempre", "no": "Nunca"
})

obesidade["Meio de Transporte"] = obesidade["Meio de Transporte"].replace({
    "Walking": "Caminhando", "Bike": "Bicicleta", "Motorbike": "Motocicleta",
    "Public_Transportation": "Transporte P√∫blico", "Never": "Nunca", "Automobile": "Autom√≥vel"
})

# @title
obesidade.head()

# @title Visualiza√ß√£o Explorat√≥ria: Entendendo o Perfil dos Pacientes

# Limpamos e traduzimos os dados.
# Agora, √© hora de ajudar a equipe m√©dica a visualizar os padr√µes presentes na base:
# Qual o perfil predominante? Existe correla√ß√£o entre vari√°veis como IMC, idade, sedentarismo?
# Essa explora√ß√£o visual serve como base para decis√µes cl√≠nicas e para o desenho do modelo preditivo.

import matplotlib.pyplot as plt
import seaborn as sns

# Contagem de registros por classe
distribuicao_obesidade = obesidade["N√≠vel de Obesidade"].value_counts()

# Exibindo em formato de tabela para refer√™ncia r√°pida
print("\nüéØ Distribui√ß√£o de N√≠veis de Obesidade:")
print(distribuicao_obesidade)

# Gr√°fico de barras horizontais
plt.figure(figsize=(10, 6))
sns.countplot(
    y="N√≠vel de Obesidade",
    hue="N√≠vel de Obesidade",
    data=obesidade,
    order=distribuicao_obesidade.index,
    palette="viridis",
    legend=False  # Evita legenda redundante
)

plt.title("Distribui√ß√£o dos N√≠veis de Obesidade na Base de Pacientes")
plt.xlabel("N√∫mero de Pacientes")
plt.ylabel("N√≠vel de Obesidade")
plt.tight_layout()
plt.show()

# @title Mapa de Correla√ß√£o entre vari√°veis num√©ricas

# Aqui buscamos identificar rela√ß√µes fortes entre vari√°veis como peso, altura, idade e IMC.
# Essas correla√ß√µes ser√£o fundamentais para a engenharia de features e para a sele√ß√£o de vari√°veis no modelo.
# Selecionar apenas colunas num√©ricas

numerical_cols = obesidade.select_dtypes(include=['float64', 'int64'])

# Matriz de correla√ß√£o
plt.figure(figsize=(12, 8))
sns.heatmap(numerical_cols.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correla√ß√£o entre vari√°veis num√©ricas")
plt.show()

# @title C√°lculo do IMC

# O √çndice de Massa Corp√≥rea (IMC) √© uma m√©trica amplamente utilizada por profissionais de sa√∫de
# para avaliar a propor√ß√£o entre peso e altura de um paciente.
# Ao incluir essa feature, traduzimos uma rela√ß√£o fisiol√≥gica conhecida em uma vari√°vel
# que pode ser diretamente interpretada e usada pelo modelo preditivo.

obesidade["IMC"] = obesidade["Peso (kg)"] / (obesidade["Altura (m)"] ** 2)

# Exibir as primeiras linhas com a nova coluna
obesidade[["Peso (kg)", "Altura (m)", "IMC"]].head()

#@title An√°lise Visual: Distribui√ß√£o do IMC por N√≠vel de Obesidade

# Esta visualiza√ß√£o √© valiosa para os profissionais de sa√∫de e para n√≥s, como cientistas de dados:
# mostra como o IMC varia entre as diferentes classifica√ß√µes cl√≠nicas de obesidade.
# O boxplot nos permite identificar sobreposi√ß√£o entre classes, outliers e poss√≠veis pontos de corte.

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.boxplot(
    x="N√≠vel de Obesidade",
    y="IMC",
    hue="N√≠vel de Obesidade",        # necess√°rio para uso de palette sem warning
    data=obesidade,
    palette="viridis",
    legend=False                     # evita legenda repetida
)

plt.xlabel("N√≠vel de Obesidade")
plt.ylabel("√çndice de Massa Corp√≥rea (IMC)")
plt.title("IMC por Classe de Obesidade")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# @title Prepara√ß√£o para Modelagem: Separando Vari√°veis
# Agora que enriquecemos e limpamos os dados, vamos separar:
# X: vari√°veis de entrada (idade, IMC, h√°bitos, etc.)
# y: vari√°vel alvo ‚Äî o n√≠vel de obesidade do paciente.
#
# Essa separa√ß√£o prepara os dados para um modelo de aprendizado supervisionado,
# um tipo de Machine Learning no qual o algoritmo aprende com exemplos rotulados ‚Äî
# ou seja, j√° sabemos a resposta correta durante o treinamento.
#
# Depois de treinado, o modelo poder√° prever o n√≠vel de obesidade para novos pacientes
# com base apenas nas informa√ß√µes de entrada (X).

X = obesidade.drop(columns=["N√≠vel de Obesidade"])  # ou selecionar apenas colunas num√©ricas relevantes
y = obesidade["N√≠vel de Obesidade"]

#@title Divis√£o Treino/Teste com reprodutibilidade

# Para garantir que o modelo aprenda de forma equilibrada,
# Separamos os dados em treino e teste mantendo a propor√ß√£o entre as classes (estratifica√ß√£o),
# o que garante uma avalia√ß√£o justa do modelo.
# Usamos uma seed fixa para que os resultados sejam reprodut√≠veis e consistentes,
# permitindo valida√ß√£o confi√°vel por outros profissionais ou institui√ß√µes.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=161651  # Garantindo reprodutibilidade e consist√™ncia nos testes
    # o uso de um n√∫mero fixo, garante que todos os processos que envolvem sorteios ou aleatoriedade sempre retornem os mesmos resultados.
    # Sem o random_state=161651, a divis√£o muda a cada execu√ß√£o. Com ele, a divis√£o dos dados ser√° sempre igual, o que permite:
    # Comparar modelos de forma justa// Replicar seus experimentos depois// Compartilhar seu c√≥digo com algu√©m e garantir os mesmos resultados
)

# @title Pipeline de Transforma√ß√£o: OneHot + MinMaxScaler

# As vari√°veis categ√≥ricas, como "Meio de Transporte" e "Lanches entre Refei√ß√µes",
# precisam ser codificadas para que possam ser entendidas pelo modelo.
# J√° as vari√°veis num√©ricas, como "Idade", "IMC" e "√Ågua Di√°ria", devem ser normalizadas para manter propor√ß√µes compat√≠veis.
# Aqui usamos um ColumnTransformer para automatizar esse pipeline.

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Identificando colunas categ√≥ricas e num√©ricas
colunas_categoricas = X.select_dtypes(include='object').columns.tolist()
colunas_numericas = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Criando o pipeline de pr√©-processamento
preprocessador = ColumnTransformer(transformers=[
    ('cat', OneHotEncoder(handle_unknown='ignore'), colunas_categoricas),
    ('num', MinMaxScaler(), colunas_numericas)
])

# Ajustando aos dados de treino
preprocessador.fit(X_train)

# @title Transforma√ß√£o e Visualiza√ß√£o dos Dados Processados

# Ap√≥s aplicar as transforma√ß√µes, criamos um novo DataFrame com os nomes finais das colunas.
# Isso nos permitir√° treinar modelos com dados j√° preparados e inspecionar como ficaram as representa√ß√µes
# ap√≥s codifica√ß√£o e normaliza√ß√£o.

# Obtendo nomes das novas colunas
nomes_cat = preprocessador.named_transformers_['cat'].get_feature_names_out(colunas_categoricas)
nomes_finais = list(nomes_cat) + colunas_numericas

# Transformando os dados
X_train_transformado = preprocessador.transform(X_train)

# Convertendo para array denso, se necess√°rio
if hasattr(X_train_transformado, "toarray"):
    X_train_transformado = X_train_transformado.toarray()

# Criando o DataFrame final
import pandas as pd
df_transformado = pd.DataFrame(X_train_transformado, columns=nomes_finais)

# Exibindo amostra
print("\n Amostra dos dados transformados:")
df_transformado.head()

# @title Modelagem Preditiva com Random Forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import cross_val_score
import joblib
import numpy as np

# Transformar os dados de teste com o mesmo pipeline de pr√©-processamento
X_teste_transformado = preprocessador.transform(X_test)

# Converter para array denso, se necess√°rio
if hasattr(X_teste_transformado, "toarray"):
    X_teste_transformado = X_teste_transformado.toarray()

# Treinamento do modelo preditivo
modelo_obesidade = RandomForestClassifier(random_state=42)
modelo_obesidade.fit(X_train_transformado, y_train)

# Previs√£o com os dados de teste
y_predito = modelo_obesidade.predict(X_teste_transformado)

# Avalia√ß√£o do modelo
print("Acur√°cia no conjunto de teste:", accuracy_score(y_test, y_predito))
print("\nRelat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_predito))
print("Matriz de Confus√£o:\n", confusion_matrix(y_test, y_predito))

# Valida√ß√£o cruzada (5 folds) para avaliar robustez
acuracias_cv = cross_val_score(modelo_obesidade, X_train_transformado, y_train, cv=5)
print("Acur√°cias (Valida√ß√£o Cruzada):", acuracias_cv)
print("Acur√°cia M√©dia na CV:", acuracias_cv.mean())

# Salvamento do modelo e do pr√©-processador para uso no Streamlit
joblib.dump(modelo_obesidade, 'modelo_obesidade.pkl')
joblib.dump(preprocessador, 'preprocessador.pkl')

"""Modelagem preditiva com Regress√£o Log√≠sitca"""

# @title Modelagem Preditiva com Regress√£o Log√≠stica
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import joblib
import numpy as np

# Transformar os dados de teste com o mesmo pipeline de pr√©-processamento
X_teste_transformado = preprocessador.transform(X_test)

# Converter para array denso, se necess√°rio
if hasattr(X_teste_transformado, "toarray"):
    X_teste_transformado = X_teste_transformado.toarray()

# Treinamento do modelo preditivo com Regress√£o Log√≠stica
modelo_obesidade = LogisticRegression(
    random_state=42,
    max_iter=1000,
    solver='lbfgs'  # padr√£o eficiente: l2 regularization
)
modelo_obesidade.fit(X_train_transformado, y_train)

# Previs√£o com os dados de teste
y_predito = modelo_obesidade.predict(X_teste_transformado)

# Avalia√ß√£o do modelo
print("Acur√°cia no conjunto de teste:", accuracy_score(y_test, y_predito))
print("\nRelat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_predito))
print("Matriz de Confus√£o:\n", confusion_matrix(y_test, y_predito))

# Valida√ß√£o cruzada (5 folds) para avaliar robustez do modelo
acuracias_cv = cross_val_score(
    modelo_obesidade,
    X_train_transformado,
    y_train,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
print("Acur√°cias (Valida√ß√£o Cruzada):", acuracias_cv)
print("Acur√°cia M√©dia na CV:", acuracias_cv.mean())

# @title Modelagem Preditiva com √Årvore de Decis√£o
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import joblib

# Transformar os dados de teste com o pipeline de pr√©‚Äëprocessamento
X_teste_transformado = preprocessador.transform(X_test)
if hasattr(X_teste_transformado, "toarray"):
    X_teste_transformado = X_teste_transformado.toarray()

# Treinamento do modelo com Decision Tree
modelo_obesidade = DecisionTreeClassifier(
    criterion='gini',        # crit√©rio de divis√£o (padr√£o: gini ou 'entropy')
    max_depth=None,          # profundidade m√°xima (None = sem limita√ß√£o)
    min_samples_split=2,     # m√≠nimo de amostras para dividir
    min_samples_leaf=1,      # m√≠nimo de amostras em uma folha
    random_state=42          # semente para reprodutibilidade
)
modelo_obesidade.fit(X_train_transformado, y_train)

# Previs√£o nos dados de teste
y_predito = modelo_obesidade.predict(X_teste_transformado)

# Avalia√ß√£o do modelo
print("Acur√°cia no conjunto de teste:", accuracy_score(y_test, y_predito))
print("\nRelat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_predito))
print("Matriz de Confus√£o:\n", confusion_matrix(y_test, y_predito))

# Valida√ß√£o cruzada (5 folds) para medir robustez
acuracias_cv = cross_val_score(
    modelo_obesidade,
    X_train_transformado,
    y_train,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
print("Acur√°cias (Valida√ß√£o Cruzada):", acuracias_cv)
print("Acur√°cia M√©dia na CV:", acuracias_cv.mean())


from sklearn.model_selection import GridSearchCV

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5]
}

grid = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid.fit(X_train_transformado, y_train)

print("Melhores par√¢metros:", grid.best_params_)
print("Melhor acur√°cia (CV):", grid.best_score_)
modelo_obesidade = grid.best_estimator_

# @title Modelagem Preditiva Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import joblib

# Transformar os dados de teste com o pipeline de pr√©-processamento
X_teste_transformado = preprocessador.transform(X_test)
if hasattr(X_teste_transformado, "toarray"):
    X_teste_transformado = X_teste_transformado.toarray()

# Treinamento do modelo com Gradient Boosting
modelo_obesidade = GradientBoostingClassifier(
    loss='log_loss',         # usa log loss (classifica√ß√£o multiclasse ou bin√°ria)
    learning_rate=0.1,       # taxa de aprendizado (shrinkage)
    n_estimators=100,        # n√∫mero de √°rvores
    max_depth=3,             # profundidade m√°xima das √°rvores
    random_state=42
)
modelo_obesidade.fit(X_train_transformado, y_train)

# Previs√£o com os dados de teste
y_predito = modelo_obesidade.predict(X_teste_transformado)

# Avalia√ß√£o do modelo
print("Acur√°cia no conjunto de teste:", accuracy_score(y_test, y_predito))
print("\nRelat√≥rio de classifica√ß√£o:\n", classification_report(y_test, y_predito))
print("Matriz de confus√£o:\n", confusion_matrix(y_test, y_predito))

# Valida√ß√£o cruzada (5 folds) para avaliar robustez
acuracias_cv = cross_val_score(
    modelo_obesidade,
    X_train_transformado,
    y_train,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
print("Acur√°cias (Valida√ß√£o Cruzada):", acuracias_cv)
print("Acur√°cia M√©dia na CV:", acuracias_cv.mean())

# @title Comparando cada Modelo
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

# ... (pr√©-processamento e defini√ß√£o dos dados conforme antes)

# Transforma√ß√£o dos dados
X_train_tr = preprocessador.transform(X_train)
X_test_tr = preprocessador.transform(X_test)
if hasattr(X_train_tr, "toarray"):
    X_train_tr = X_train_tr.toarray()
if hasattr(X_test_tr, "toarray"):
    X_test_tr = X_test_tr.toarray()

# Defini√ß√£o dos modelos
modelos = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

nomes = []
medias_cv = []
acc_tests = []

for nome, modelo in modelos.items():
    scores = cross_val_score(modelo, X_train_tr, y_train, cv=5, scoring='accuracy', n_jobs=-1)
    modelo.fit(X_train_tr, y_train)
    acc_test = accuracy_score(y_test, modelo.predict(X_test_tr))
    nomes.append(nome)
    medias_cv.append(scores.mean())
    acc_tests.append(acc_test)

# Plot com layout ajustado e anota√ß√µes centralizadas
fig, ax = plt.subplots(figsize=(9, 5), layout='constrained')

x = np.arange(len(nomes))
width = 0.35

bars_cv = ax.bar(x - width/2, medias_cv, width, label='CV Mean Accuracy')
bars_test = ax.bar(x + width/2, acc_tests, width, label='Test Accuracy')

ax.set_xlabel('Modelos')
ax.set_ylabel('Acur√°cia')
ax.set_title('Compara√ß√£o de modelos: Acur√°cia CV vs Teste', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(nomes, rotation=30, ha='right')
ax.set_ylim(0, 1)
ax.legend(loc='upper left')

# Adicionar etiquetas sobre as barras
ax.bar_label(bars_cv, fmt='%.2f', padding=3)
ax.bar_label(bars_test, fmt='%.2f', padding=3)

plt.show()

!pip show scikit-learn